{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1748c2cb684655f5b0503b180cbf1e8",
     "grade": false,
     "grade_id": "cell-21c638cff1c06e0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"2.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c0fbe528d3cf3da76f520d0676c1e97",
     "grade": false,
     "grade_id": "cell-e3c1b1cefbe371a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='toc'></a>\n",
    "# Table of Contents\n",
    "- **[Assignment Description](#Topic0)**\n",
    "- **[Topic 1 - K-means clustering on text documents](#Topic1)**\n",
    "  - [Task 1](#t1)\n",
    "- **[Topic 2 - Cluster Labeling](#Topic2)**\n",
    "  - [Task 2](#t2)\n",
    "- **[Topic 3 - Hierarchical clustering](#Topic3)**\n",
    "  - [Task 3](#t3)\n",
    "- **[Topic 4 - Clustering, Feature Types, and Representations](#Topic4)**\n",
    "  - [Task 4a](#t4a)\n",
    "  - [Task 4b](#t4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings only when absolutely necessary\n",
    "# Warnings are in place for a reason!\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab31f72f69bdeb114a81e5956c290524",
     "grade": false,
     "grade_id": "cell-e2eac58d776dd183",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic0'></a>\n",
    "## SIADS 543 Assignment 2: Clustering\n",
    "\n",
    "In this week's assignment you'll gain experience applying different clustering methods, computing cluster labels, and selecting an optimal number of clusters based on quality assessment metrics. In addition, you'll gain experience working with text data. \n",
    "\n",
    "We'll continue to use the \"beer\" dataset, but this time focused more on the *text* of beer reviews. In the \"bag of words\" scenario, each review is represented by a sparse vector that's filled in according to which specific terms are found in that review. (As a reminder, we generally use \"term\" to refer to a word or phrase that's part of a text representation - since as you'll see below, when working with text it's very useful to work with phrases as features as well as individual words.) In particular, we'll treat single terms and two-term bigrams (phrases) as features here. You'll apply a simple Vectorizer to process the text, which will also be a useful prelude to next week's upcoming assignment on topic models. To familiarize yourself with how a text passage can be represented as a vector of word counts or weights, the \"bag of words\" model, *please read Sec. 7.3-7.5 in the textbook*. \n",
    "\n",
    "As usual, please read through the entire assignment before starting, to get an idea of how the questions relate to each other and the overall goals.\n",
    "\n",
    "*Please note that for autograder messages that check a list, it will report any problems using a list index starting at zero, i.e. the first list element is called \"element 0\".*  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea6bf86c94a274c26392d9053a863769",
     "grade": false,
     "grade_id": "cell-76bdf7beb7618c48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First import some necessary libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "up, down = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "## Additional imports can be inlcuded here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f920b346d84e8f1c19d665d3f42cac64",
     "grade": false,
     "grade_id": "cell-70a89862de7515a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic1'></a>\n",
    "### Topic 1 - K-means clustering on text documents\n",
    "\n",
    "In this question you'll apply k-means clustering to a set of text documents to gain some insight into the nature of the content. We're going to use the same beer dataset as last week, but this time we'll focus on the 'text' column that contains written reviews of the tasty beverage.\n",
    "\n",
    "This scenario uses the TfidfVectorizer class (described in Sec. 7.5), to convert each textual review to a numeric vector. We're going to cover text processing in more detail next week, including TfidfVectorizer and its many parameters, but for now we've provided you the code that reads in the dataset, and converts each text review into a sparse numeric vector. \n",
    "\n",
    "A general rule when clustering text documents is that *noun phrases* work well as features. For now, we'll ignore parts of speech, but we will use both single words and *bigrams* (two-word phrases) as features. This is easy to do with the Vectorizer by setting the ngram_range property to (1,2).  \n",
    "\n",
    "A big problem in applying $K$-means (and many other unsupervised clustering-type methods) is that we need to specify the number of clusters $K$ in advance - but we don't know in advance what value of $K$ will give the best-quality clustering!  So, to pick the \"best\" number of clusters we're going to rely on calculating some automated measures of cluster quality. You'll compute these cluster quality measures for each choice of $K$, and then pick the value of $K$ that gives the best value of the quality measure(s).\n",
    "\n",
    "We seen a few ways to assess cluster quality, including the silhouette score, but we're going to try out two alternative cluster quality measures that you should be aware of. They reward different ideas of \"quality\" for a clustering, but like the silhouette score, do not require ground-truth labels to compute quality (which is good, because we don't have them). These two measures are:\n",
    "\n",
    "The Davies-Bouldin score https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html \n",
    "*Lower* is better for the Davies-Bouldin score.\n",
    "\n",
    "The Calinski-Harabasz index  https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index\n",
    "*Higher* is better for the Calinski-Harabasz index.\n",
    "\n",
    "We're looking for clusterings that have a *high* Calinski-Harabasz index, but a *low* Davies-Bouldin score.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b207df367c3b1a304e4524e0d00df2fd",
     "grade": false,
     "grade_id": "cell-7d16864d6773e96a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This function returns:\n",
    "# - a matrix X with one row per document (review). Each row is a sparse\n",
    "# vector containing tf.idf term weights for the words in the document.\n",
    "#\n",
    "# - the vectorizor used to create X\n",
    "#\n",
    "# - the actual reviews used as input to the vectorizer\n",
    "\n",
    "\n",
    "def get_beer_reviews_vectorized(top_n=-1, ngram_range=(1, 1), max_features=1000):\n",
    "    df = pd.read_csv(\"./assets/beer2.csv\")[\"text\"]\n",
    "    df = df.dropna()  # drop any rows with empty reviews\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.5,\n",
    "        max_features=max_features,\n",
    "        min_df=2,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=ngram_range,\n",
    "        use_idf=True,\n",
    "    )\n",
    "    if top_n >= 0:\n",
    "        review_instances = df.values[0:top_n]\n",
    "    else:\n",
    "        review_instances = df.values\n",
    "\n",
    "    X = vectorizer.fit_transform(review_instances)\n",
    "\n",
    "    return (X, vectorizer, review_instances)\n",
    "\n",
    "\n",
    "def print_cluster_features(vectorizer, centroids, n_clusters, top_n_features):\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i, end=\"\")\n",
    "        for ind in centroids[i, :top_n_features]:\n",
    "            print(\" [%s]\" % terms[ind], end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46eb875d6ce391d9aedf46d567f97d0c",
     "grade": false,
     "grade_id": "cell-c1fa2b942ac6907f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Execute the following cell to preprocess the dataset.\n",
    " - For efficiency reasons we're going to sample a subset of reviews\n",
    " - For performance reasons, this question will use only the first 5000 reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8ff9a4987d49a2290618c3ff342a8a5",
     "grade": false,
     "grade_id": "cell-04167b08e2417118",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(X, vectorizer, review_instances) = get_beer_reviews_vectorized(5000, (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "386e7ed149f31241e18a847cf8b91c7e",
     "grade": false,
     "grade_id": "cell-3fae554db312bd4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t1'></a>\n",
    "### Task 1 - K-means Review  (25 points)\n",
    "\n",
    "Okay - here's what you need to do for Question 1:\n",
    "\n",
    "1. Get the vectorized reviews using the get_beer_reviews_vectorized() function as shown below. For speed reasons we will only analyze a sample of 5000 reviews and not the whole set. The vectorized reviews will be output to the matrix X, which should have 5000 rows (one per review) and 1000 columns (one per word/term in the vocabulary produced by the vectorizer).\n",
    "\n",
    "2. For each choice of K from 2 to 9, run k-means clustering on these reviews. For each K, compute the above two cluster quality scores on the resulting clustering. IMPORTANT: You won't be including the code to run the whole loop in your autograded code submission: you're just trying to find the optimal value for $K$ for now.\n",
    "\n",
    "IMPORTANT: *when running KMeans, set init='k-means++', max_iter=100, n_init=1, and random_state=42.*\n",
    "\n",
    "After you perform your clustering run in Step 2, you should see there's one clearly superior choice for K that gives the best value for both metrics (high Calinski-Harabasz score, low Davies-Bouldin score). \n",
    "\n",
    "3. Now you will write your assignment code that does a single clustering using that optimal K, and finds what the predominant terms are in the resulting clusters.  Remember, each cluster is a collection of vectors, each of which represents a beer review. To find good representative terms for each cluster, you want to know what the \"typical\" review looks like for each of the K clusters. A \"typical\" review for a cluster is just the mean of all the vectors (reviews) that belong to that cluster. This vector is known as the cluster centroid or center. *Note that when you run K-Means you don't have to compute the cluster centers yourself -- they are computed for you by K-Means after you run 'fit'. Just use the resulting cluster_centers_ property.*\n",
    "\n",
    "4. The cluster center, since it's a just the mean of a bunch of review vectors, is itself just a vector of the same dimension as the input review vectors: think of the cluster center vector like a \"word cloud\" that holds the weight of each possible word in a review. So to get the most predominant terms in a vector, just sort the entries of the cluster centroid *from highest to lowest term weight*, and return the corresponding term strings. *You can do this sorting with one line of code, on all the cluster centers simultaneously, by applying argsort(..) to the cluster_centers_ array.*\n",
    "\n",
    "Take a look at the list of top-weighted terms that come out for each cluster: do they look reasonable? Are the terms in each cluster similar in some way to each other? (We will revisit these terms in the next question.)  Remember that you can get the term strings that correspond to the review vector entries by using the vectorizer's get_feature_names() method.  *You can use the `print_cluster_features` example code above for reference on how to do this.*  \n",
    "\n",
    "Your function should return a list that has $K$ elements, where $K$ is the optimal number of clusters you found above. Each of these $K$ elements should itself contain a list of the top 10 terms (strings) for that cluster. These terms should be sorted by highest to lowest term weight value as stored in the cluster centroid.\n",
    "\n",
    "After you submit your solution using the parameters we supplied, we encourage you to play with different values for the parameters of TfidfVectorizer, to see how the results change, because the results of this kind of clustering can be very sensitive to those parameters, which control how the text is processed, what defines what a \"term\" is, and which words are kept as features in the clustering.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9852a8df73d3f54a0f8eae708fff975",
     "grade": true,
     "grade_id": "cell-63b2e7eb6bfcfd41",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Why is this room so dark?\n",
    "task_id = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b3fcde1d4293db6f3162ecfc35c534f",
     "grade": false,
     "grade_id": "cell-11b6922067d8ad4e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_kmeans_review():\n",
    "    from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    result = None\n",
    "    d_davies = {}\n",
    "    d_calinski = {}\n",
    "    \n",
    "    for i in range(2, 10):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        d_davies[i] = davies_bouldin_score(X.toarray(), kmeans.labels_)\n",
    "        d_calinski[i] = calinski_harabasz_score(X.toarray(), kmeans.labels_)\n",
    "        \n",
    "    optimal_k = max(d_calinski, key=d_calinski.get)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=100, n_init=1, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "    clusters = []\n",
    "    \n",
    "    for i in range(0, optimal_k):\n",
    "        cluster_terms = [ind for ind in np.argsort(kmeans.cluster_centers_[i])[::-1][:10]]\n",
    "        clusters.append(cluster_terms)\n",
    "        \n",
    "    result = clusters\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2639700244744dfc9ca8e28636f505ce",
     "grade": true,
     "grade_id": "cell-17e8ddcf0455395f",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 - AG tests\n",
      "Task 1 - your answer:\n",
      "[[160, 183, 225, 777, 97, 127, 868, 629, 548, 578], [629, 474, 397, 539, 887, 578, 548, 88, 462, 975]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_kmeans_review()\n",
    "\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(\n",
    "    stu_ans, list\n",
    "), \"Task 1: Your function should return a list (of string lists). \"\n",
    "assert np.array(\n",
    "    [type(elt) == list for elt in stu_ans]\n",
    ").all(), \"Task 1: each cluster summary should be a list (of terms).\"\n",
    "assert np.array(\n",
    "    [len(elt) == 10 for elt in stu_ans]\n",
    ").all(), \"Task 1: each cluster summary should have exactly 10 terms.\"\n",
    "\n",
    "\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d90ed0b0ce24281db5e9f131ef4ac5a3",
     "grade": false,
     "grade_id": "cell-496c784cd5a58f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic2'></a>\n",
    "## Topic 2 - Cluster Labeling\n",
    "\n",
    "It was fun to extract the words in the 'typical' review for each cluster using the cluster centroid. However, you may have noticed that some words were duplicated across clusters and weren't that specific to a single cluster. For example 'nice' and 'like' appear in more than one 'typical' review, which makes sense given that reviews express opinions no matter which cluster a brew might appear in.  \n",
    "\n",
    "Ideally, we'd like to find slighly better representative words for each cluster that are somewhat *specific* to that cluster and not others, i.e. words that distinguish reviews in that cluster from reviews in other clusters. To use the technical term, we want terms that are *discriminative* with respect to the clusters, not just descriptive.\n",
    "\n",
    "Recall that in dimensionality reduction we saw information gain (IG) as a feature selection criterion: it helps find features that distinguish one class from another class, improving prediction accuracy. Even though we don't have labels, we can use the same idea to find terms that are *special* to that cluster, that distinguish that cluster from the others. These IG-based distinctive terms will be our new cluster \"summary\".  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1adad0bda5fe2cf8f9177c93c2e50e74",
     "grade": false,
     "grade_id": "cell-83fb73a95c068d36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t2'></a>\n",
    "### Task 2 - Cluster Labeling - Improving Cluster Representations (25 points)\n",
    "\n",
    "To obtain these improved terms, implement the following steps.  These instructions are a bit long, but that's to make the process as clear as possible.\n",
    "\n",
    "1. Run k-means again (using same algorithm settings as Q1) with the same beer review collection, **but this time setting k=7 (seven) to give a slightly more diverse set of clusters to start with**.\n",
    "\n",
    "\n",
    "2. Get the cluster-term matrix of cluster centers $L$ from the result of k-means.  This should be an array with seven (7) rows, one row per cluster, and 1000 columns (one column per word/term in the vocabulary produced by the vectorizer).\n",
    "\n",
    "\n",
    "3. For each row $c$ in $L$,  (i.e. for each cluster)\n",
    "\n",
    "    a. For this cluster $c$, we'll compute a **'distinctive term score'** for each of the terms/words in the vocabulary, to see how specific it is to this particular cluster. To do this, we're going to compute how surprised we are at the distribution $T_w$ of a word $w$ between this cluster vs all other clusters, compared to the distribution $T_c$ of the average word between this cluster $c$ and all clusters. If a word is a lot more likely to occur in this cluster than we would expect by chance, it is considered a more distinctive word for this cluster. This surprise factor is *information gain*.  (The instructors will provide a link to a mini-lecture on information gain as a supplemental resource.)\n",
    "\n",
    "    So far, we've introduced two terms, $T_w$, and $T_c$.\n",
    "\n",
    "    |   | Term    | Description  |\n",
    "    |:--|:--------|:-------------|\n",
    "    | 1 | $T_w$   | Distribution of the word $w$ in cluster $c$ vs. *all other clusters* |\n",
    "    | 2 | $T_c$   | Distribution of the *average word* in cluster $c$ vs. *all clusters* |\n",
    " \n",
    "    \n",
    "4. More formally, we define the variable $T_c$ to have two possible values: 'in this cluster $c$' and 'in any other cluster'. Each of these outcomes has a probability when we are talking about where words occur. Thus, $T_c$ is a numpy array of shape (2, ). For example, suppose we pick a word from the vocabulary at random and find that on average, $T_c$ = `[0.25, 0.75]`. In other words, the chance on average that a word occurs in this cluster $c$ is `25%`, and the chance of occurring in any other cluster is `75%`.\n",
    "\n",
    "  This additional information about the shape should make the description of $T_c$ a bit clearer.\n",
    "\n",
    "|   | Term    | Description  |  Shape      |\n",
    "|:--|:--------|:-------------|:------------|\n",
    "| 1 | $T_c$   | Distribution of the *average word in cluster $c$* vs. *all clusters* | (2, ) |\n",
    "\n",
    "\n",
    "5. We have provided a function for you that makes it easy to compute $T_c$, or other statistics of words in this cluster $c$ compared to all other clusters. This is the provided function 'one_vs_all_count_matrix'. As input, you provide the matrix $L$ along with the row $c$ you want to analyze. The output is a \"one versus all\" matrix $M$ that has shape `(2, 1000)`: the first row has the weights of words in *this* cluster $c$ (which is the same as `L[c, :])`, and the second row has the aggregate summed weights of the words across all *other* clusters.  The columns are the same as $L$: one column per term in the vocabulary.\n",
    "    \n",
    "    The shape of $M$ should remind you of $T_c$ and its purpose.\n",
    "    \n",
    "  |   | Term    | Description  |  Shape      |\n",
    "  |:--|:--------|:-------------|:-----------:|\n",
    "  | 3 | $M$     | Class-Term Matrix from `one_vs_all_count_matrix` giving weights of cluster $c$ and all other clusters | `(2, 1000)` |\n",
    " \n",
    "  a. To compute $T_c$, just compute $M$ and sum over all columns (`axis = 1`) so that you get a `(2, )` array. Then to turn it into a probability distribution, normalize by computing `T_c = T_c / sum(T_c)`.  \n",
    "  \n",
    "  b. Now loop through each column (term) $i$ of $L$. We'll call the term corresponding to the $i$-th column, term $w$. So for each term $w$ you'll compute the second part we need: the term-specific distribution $T_w$ for that specific term $w$ in this cluster vs all clusters. \n",
    "    \n",
    "  To compute $T_w$ is very simple: it's just the $i$-th column of the one-vs-all matrix $M$ (term weight in this cluster, term weight in all other clusters) but again normalized to turn it into a 2-element probability distribution by computing `T_w = T_w / sum(T_w)`.  Like $T_c$, this should be an array of shape `(2, )`. \n",
    "    \n",
    "    To recap:\n",
    "    \n",
    "  |   | Term    | Description  |  Shape      |\n",
    "  |:--|:--------|:-------------|:-----------:|\n",
    "  | 1 | $T_w$   | Distribution of the word $w$ in cluster $c$ vs. *all other clusters* | `(2, )` |\n",
    "  | 2 | $T_c$   | Distribution of the *average word in cluster $c$* vs. *all clusters* | `(2, )` |\n",
    "  | 3 | $M$     | Class-Term Matrix from `one_vs_all_count_matrix` giving weights of cluster $c$ and all other clusters | `(2, 1000)` |\n",
    "    \n",
    "  Then use the provided `compute_distinctive_term_score` function to compute the information gain for that term. The first argument is the two-element probability distribution $T_c$ you computed for all words, and the second argument is the two-element term-specific probability distribution $T_w$ just for this term $w$.  Append the resulting score to a result list.\n",
    "    \n",
    "  c. After looping through all terms in (c) above, you should have a result list with 1000 entries, one per term, containing the distinctive score for each term, for that cluster $c$.\n",
    "    \n",
    "  d. Get the most distinctive terms for this cluster $c$ by sorting your array by descending score and selecting the top 5.\n",
    " \n",
    " \n",
    "6. Your function should return a list of string lists: the main list will contain, for each cluster, a string list containing the top *five* terms, sorted from highest to lowest modified distinctive term score.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bde75f7ca7ac4495496c4ef3f005e85",
     "grade": false,
     "grade_id": "cell-bde6497914398032",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def compute_distinctive_term_score(T, T_a):\n",
    "    # First compute information gain.\n",
    "    IG = entropy(T) - entropy(T_a)\n",
    "\n",
    "    # if it's high IG, but not for this class, we want to penalize,\n",
    "    # so flip the IG negative.  We do this because these are terms those whose *absence* is notable,\n",
    "    # but we don't care about those for purposes of this assignment and so we give them\n",
    "    # a score that guarantees we won't rank them highly.\n",
    "    if T_a[0] < T_a[1]:\n",
    "        score = -IG\n",
    "    else:\n",
    "        score = IG\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# create a 1-vs-all two-class matrix for each cluster\n",
    "def one_vs_all_count_matrix(m, index):\n",
    "    # row zero is the selected row\n",
    "    row0 = m[index, :]\n",
    "    # row one is the other rows summed\n",
    "    row1 = np.vstack((m[0:index, :], m[index + 1 :, :])).sum(axis=0)\n",
    "\n",
    "    result = np.vstack((row0, row1))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac09cf9043b923f87d5a633c106a77bc",
     "grade": true,
     "grade_id": "cell-f8359bcfdbc64bda",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If these walls could talk...\n",
    "task_id = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1b6063767b75cdbc36e5a654ad43db7",
     "grade": false,
     "grade_id": "cell-f71840b128d02b5c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_cluster_labeling():\n",
    "    result = None\n",
    "    from sklearn.cluster import KMeans\n",
    "    k = 7\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    L = kmeans.cluster_centers_\n",
    "    dic_new = {}\n",
    "    for clus in range(k):\n",
    "        M = one_vs_all_count_matrix(L, clus)\n",
    "        tc = M.sum(axis=1) / M.sum()\n",
    "        tw = M / M.sum(axis=0)\n",
    "        lis = []\n",
    "        for i in range(1000):\n",
    "            lis.append(compute_distinctive_term_score(tc, tw[:, i]))\n",
    "        dic_new[clus] = [lis.index(x) for x in sorted(lis, reverse=True)[:5]]\n",
    "    result = []\n",
    "    for clus in range(k):\n",
    "        result.append([terms[j] for j in dic_new[clus]])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98403b8b29763855a08ddcd14b71f0e9",
     "grade": true,
     "grade_id": "cell-9175ae40475d7568",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 - AG tests\n",
      "Task 2 - your answer:\n",
      "[['chocolate', 'bittersweet', 'black', 'colored head', 'roast'], ['pilsner', 'small white', 'peppery', 'pale malt', 'tartness'], ['pumpkin pie', 'pumpkin ale', 'pumpkin beer', 'pumpkin', 'orange color'], ['chocolate malt', 'brown head', 'roasted', 'licorice', 'roasty'], ['hopped', 'hop', 'grassy hops', 'hop bite', 'grassy'], ['cherries', 'reddish', 'barrel', 'plum', 'vintage'], ['bar', 'corn', 'marzen', 'adjunct', 'sam']]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_cluster_labeling()\n",
    "\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(\n",
    "    stu_ans, list\n",
    "), \"Task 2: Your function should return a list (of string lists). \"\n",
    "\n",
    "assert np.array(\n",
    "    [type(elt) == list for elt in stu_ans]\n",
    ").all(), \"Task 2: each cluster summary should be a list (of terms).\"\n",
    "\n",
    "assert np.array(\n",
    "    [len(elt) == 5 for elt in stu_ans]\n",
    ").all(), \"Task 2: each cluster summary should have exactly 5 terms.\"\n",
    "\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "attachments": {
    "dendrogram_labeled.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c39f91dba033db9afcbce4b8f52e421",
     "grade": false,
     "grade_id": "cell-84edfb37a56d65fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic3'></a>\n",
    "## Topic 3 - Hierarchical clustering\n",
    "\n",
    "Now we're going to use the beer names that correspond with the reviews to do a little hierarchical beer clustering according to their review similarity (based on the bag of words model used in the first question).\n",
    "\n",
    "We're actually going to rely on scipy for this question: specifically, scipy.cluster.hierarchy and its functions `ward(.)` and `dendrogram(.)`.\n",
    "\n",
    "Note that the cluster-term matrix returned by the vectorizer is sparse. However, we need a dense array to use the ward() function, so we densify the array, i.e. cls = ward(X.toarray())\n",
    "\n",
    "The structure returned by `ward` contains the linkage structure computed by the bottom-up, agglomerative clustering, and looks like this:\n",
    "\n",
    "`[[ 7.     8.     0.947  2.   ]\n",
    "  [ 0.     1.     1.111  2.   ]\n",
    "  [ 2.    10.     1.15   3.   ]\n",
    "  [ 5.    11.     1.204  3.   ]\n",
    "  [ 3.     6.     1.219  2.   ]\n",
    "  [ 4.    14.     1.33   3.   ]\n",
    "  [ 9.    13.     1.34   4.   ]\n",
    "  [15.    16.     1.509  7.   ]\n",
    "  [12.    17.     1.728 10.   ]]`\n",
    " \n",
    "Let's call this structure table `T`. It describes a tree, where each node of the tree has a unique integer called its \"node index\".  The columns of `T` are:\n",
    "- node index of first child\n",
    "- node index of second child\n",
    "- cluster distance at which the clusters were merged\n",
    "- number of instances in the resulting new cluster, after the merge.\n",
    "\n",
    "It's useful to understand and be able to compute things with this structure if needed. \n",
    "\n",
    "A cluster with a node index of less than N corresponds to a leaf node at the bottom of the tree, i.e. one of the original observations: the original data instances have node indexes of 0 through N-1 where N is the total number of instances (data points). They are considered to be in their own 1-instance cluster and are not included in `T`.\n",
    "\n",
    "In row i of `T`, two clusters with node indices T[i, 0] and T[i, 1] are combined to form a new cluster with node index N + i. \n",
    "The cluster algorithm's computed distance (e.g. Ward's distance) between clusters T[i, 0] and T[i, 1] is given by T[i, 2]. \n",
    "The fourth column value T[i, 3] represents the number of original observations (leaf nodes) that are in the newly formed cluster.\n",
    "\n",
    "The tree corresponding to this structure, with all node indices labeled, is shown here:\n",
    " ![dendrogram_labeled.png](attachment:dendrogram_labeled.png)  \n",
    " \n",
    " <a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37a00db92c7a753f5c24f17ec21245d0",
     "grade": false,
     "grade_id": "cell-d1e660f3bfbc7380",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plot a dendrogram of agglomerative clusters of the first 30 beer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04072013cc0dc2daae5f46688f668bca",
     "grade": false,
     "grade_id": "cell-236190fb6257d75f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(X, vectorizer, review_instances) = get_beer_reviews_vectorized(30, (1, 2), 1000)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "\n",
    "def plot_beer_reviews():\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cls = ward(X.toarray())\n",
    "    dendrogram(cls)\n",
    "    plt.show()\n",
    "    print(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efa053d97848a580cac1910cd954bb53",
     "grade": false,
     "grade_id": "cell-4d6cc66549e6879d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3'></a>\n",
    "### Task 3  (25 points)\n",
    "Okay - here's what you need to do for this question. The goal is to compare how the large clusters found by hierarchical clustering compare with the ones you found using k-means clustering.\n",
    "\n",
    "**Step 1.** Call `(X, vectorizer, review_instances) = get_beer_reviews_vectorized(1000, (1,2), 1000)` to get the review-term matrix of sparse vectors corresponding to the first 1000 reviews in the dataset (and using the top 1000 bigram terms by frequency).\n",
    "\n",
    "**Step 2.** Use the scipy wards method to cluster the instances in X.\n",
    "\n",
    "**Step 3.** Use the resulting hierarchical cluster tree to get the reviews that fall into each of K clusters, where K = 2.\n",
    "\n",
    "The easiest way to do this question is to use the \"fcluster\" function that's part of the scipy.cluster.hierarchy package.\n",
    "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "\n",
    "You should use the 'maxclust' option of fcluster to automatically find the merge distance threshold that splits the reviews into K clusters and returns the cluster label of each review.  Use K = 2 for the results you submit.\n",
    "\n",
    "**Step 4a** Write a loop that, for each cluster you found in Step 3, does the following:\n",
    "\n",
    "**Step 4b**  Use the loop index 0, 1, ... as the cluster ID. Within the loop, filter the results of `fcluster` to find only the instances that belong to the given cluster ID.  Recall that the numpy \"where\" function can return you the indices of an array that meet your condition (such as which cluster index an instance has). You can then use these indices to select all elements with a given cluster ID.\n",
    "\n",
    "**Step 4c.** Next within the loop, call the `compute_centroid` function to compute a centroid for the elements you found in Step 4b: the result will be a 'typical' review for that cluster -- essentially, a \"term cloud\" for that cluster's reviews. \n",
    "\n",
    "**Step 4d.** Next within the loop, find the top FIVE terms in the centroid having the *highest* weight (sorted by descending weight), and save the strings for those terms in a list.\n",
    "\n",
    "**Step 5.** Outside the loop, your function should return a list that contains TWO term lists (strings) corresponding to the K=2 clusters. Each term list should contain the top *five* terms in the cluster centroid you computed (sorted by highest to lowest term weight).  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc5d74118268cbfde0ad6326b44cffa3",
     "grade": false,
     "grade_id": "cell-44bde0e6ec6002a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_centroid(X, node_list):\n",
    "    \"\"\"New centroid calculation function!\n",
    "    compute_centroid:  given X, a review-term matrix of sparse vectors generated by\n",
    "    a vectorizer, and a list of row indices to extract from that matrix,\n",
    "    return the centroid of the selected vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_instances = X[node_list, :]\n",
    "\n",
    "    dense_cluster_instances = cluster_instances.todense()\n",
    "\n",
    "    centroid = np.mean(dense_cluster_instances, axis=0)\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcdf932459eb7c82f5e220329f256e42",
     "grade": true,
     "grade_id": "cell-bd9ecd23b9be9328",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Too bad there is not a key to unlock this door.\n",
    "task_id = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a0b7fcc1064a0db7c9fa5b17c84131f",
     "grade": false,
     "grade_id": "cell-cb76c12ea7536548",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_hierarchy():\n",
    "    result = []\n",
    "    from scipy.cluster.hierarchy import linkage, fcluster, ward\n",
    "    (X, vectorizer, review_instances) = get_beer_reviews_vectorized(1000, (1,2), 1000)\n",
    "    Z = ward(X.toarray())\n",
    "    cluster_id = fcluster(Z,2,criterion ='maxclust')-1\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    l = []\n",
    "    for k in range(0,2):\n",
    "        # Get the indices of reviews in cluster k\n",
    "        node_list = np.where(cluster_id == k)[0]\n",
    "        # Compute the centroid of the cluster\n",
    "        centroid = np.array(compute_centroid(X, node_list)).flatten()\n",
    "#         top_5_indices = centroid.argsort()[::-1][:5]\n",
    "        top_5_indices = np.argsort(centroid)[::-1][:5]\n",
    "        top_5_terms = [terms[i] for i in top_5_indices]\n",
    "        result.append(top_5_terms)\n",
    " \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a38d48a7295dd1fd63151d883e5f2b8",
     "grade": true,
     "grade_id": "cell-09d1290a9c6bb5d6",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3 - AG tests\n",
      "Task 3 - your answer:\n",
      "[['pumpkin', 'pie', 'pumpkin pie', 'spices', 'nutmeg'], ['nice', 'light', 'sweet', 'hops', 'good']]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_hierarchy()\n",
    "\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(\n",
    "    stu_ans, list\n",
    "), \"Q3: Your function should return a list (of string lists). \"\n",
    "\n",
    "assert np.array(\n",
    "    [type(elt) == list for elt in stu_ans]\n",
    ").all(), \"Q3: each cluster summary should be a list (of terms).\"\n",
    "\n",
    "assert np.array(\n",
    "    [len(elt) == 5 for elt in stu_ans]\n",
    ").all(), \"Q3: each cluster summary should have exactly 5 terms.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01f4f7fc88b2a8fab2d491d467790234",
     "grade": false,
     "grade_id": "cell-8d3ef189984c385b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic4'></a>\n",
    "## Topic 4 - Clustering, Feature Types, and Representations (25 points total).\n",
    "\n",
    "### Clustering with mixed feature types and cluster representations.\n",
    "\n",
    "The classic version of k-means assumes squared Euclidean distance. This assumption is used when the cluster representatives $m_k$ are assumed to be the means of the currently assigned cluster members. This is fine if all the features/variables are continuous values where taking the mean over them makes sense. However, there are many situations where we have a mixture of feature types, including categorical or ordinal features, where computing a mean doesn't make sense. In these cases, we must consider a more general version of k-means, based on an arbitrary dissimilarity function $D(x_i, x_j)$ between items that is used to compute the optimal cluster representatives $m_k$ (instead of a mean). One widely-used example of this is *k-medoids clustering*, which finds an existing item in the cluster that has minimum total distance to the other points in the cluster. (Even more generally, we can define a clustering algorithm just using proximity matrices of items, if they are available, and not even computing cluster centers: just keeping the item index of the cluster representative.)\n",
    "\n",
    "We're going use k-medoids for this question - but scikit-learn doesn't yet have a k-medoids implementation, so we're going to use a package called sklearn-extra, which is installed for you in the unsupervised learning Coursera environment. Note that k-medoid is computationally expensive: a naive implementation is quadratic in its runtime requirements (in $n$, the number of data instances) although more clever implementations can reduce that.\n",
    "\n",
    "Another problem this question deals with is how to do clustering when your data has a mix of feature types. We solve this here by computing a generalized form of distance between instances called the *Gower distance.*\n",
    "\n",
    "Here's a summary of how Gower distance is computed:  For each variable type, pick a distance that scales 0 to 1, then take a linear combination of these distance based on user-specified weights.\n",
    " \n",
    "For quantative (interval) variables: use manhattan distance.\n",
    "For ordinal variables: rank or manhattan distances, with special adjustment for ties.\n",
    "For nominal variables: variables of k categories are converted to one-hot encoding (k binary columns) and compared using the Dice coefficient.  You can read more about one-hot encodings in Chapter 4 of the course textbook (Intro to ML with Python), and the Dice coefficient here: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "\n",
    " \n",
    "For more details about the Gower distance, see: Gower, J.C., 1971, A General Coefficient of Similarity and Some of Its Properties. Biometrics.  Vol. 27, No. 4 (Dec., 1971), pp. 857-871.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b31e7270ba3043343590562debc2ccf",
     "grade": false,
     "grade_id": "cell-2f6d471c880e4b6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We're going to work with the following dataset about U.S. colleges and universities. It contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report.\n",
    "\n",
    "A data frame with 777 observations on the following 18 variables:\n",
    "\n",
    "`Private` A factor with levels No and Yes indicating private or public university\n",
    "\n",
    "`Apps` Number of applications received\n",
    "\n",
    "`Accept` Number of applications accepted\n",
    "\n",
    "`Enroll` Number of new students enrolled\n",
    "\n",
    "`Top10perc` Pct. new students from top 10% of H.S. class\n",
    "\n",
    "`Top25perc` Pct. new students from top 25% of H.S. class\n",
    "\n",
    "`F.Undergrad` Number of fulltime undergraduates\n",
    "\n",
    "`P.Undergrad` Number of parttime undergraduates\n",
    "\n",
    "`Outstate` Out-of-state tuition\n",
    "\n",
    "`Room.Board` Room and board costs\n",
    "\n",
    "`Books` Estimated book costs\n",
    "\n",
    "`Personal` Estimated personal spending\n",
    "\n",
    "`PhD` Pct. of faculty with Ph.D.’s\n",
    "\n",
    "`Terminal` Pct. of faculty with terminal degree\n",
    "\n",
    "`S.F.Ratio` Student/faculty ratio\n",
    "\n",
    "`perc.alumni` Pct. alumni who donate\n",
    "\n",
    "`Expend` Instructional expenditure per student\n",
    "\n",
    "`Grad.Rate` Graduation rate  \n",
    "\n",
    "### Be sure the following cell is executes to load the college dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eed4ef5c8287008157383391f8ee1d55",
     "grade": false,
     "grade_id": "cell-c23c8dad5c76c72d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./assets/college.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84567329d8be86ecb56d0ecfbb0efb7a",
     "grade": false,
     "grade_id": "cell-cec10316fb5b65e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Use the following gower_distances() function to compute a Gower distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b5edc0b64f6e48b04ed033787b60a3b",
     "grade": false,
     "grade_id": "cell-2f2bcdbaee3ac809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.utils import validation\n",
    "from sklearn.metrics import pairwise\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# The following functions are provided by Marcelo Beckmann and currently are part of the scikit-learn\n",
    "# github repository but have not yet been officially released.\n",
    "\n",
    "# This is a utility function used by gower_distances below.\n",
    "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\n",
    "    X, Y, dtype_float = pairwise._return_float_dtype(X, Y)\n",
    "\n",
    "    warn_on_dtype = dtype is not None\n",
    "    estimator = \"check_pairwise_arrays\"\n",
    "    if dtype is None:\n",
    "        dtype = dtype_float\n",
    "\n",
    "    if Y is X or Y is None:\n",
    "        X = Y = validation.check_array(\n",
    "            X, accept_sparse=\"csr\", dtype=dtype, estimator=estimator\n",
    "        )\n",
    "    else:\n",
    "        X = validation.check_array(\n",
    "            X, accept_sparse=\"csr\", dtype=dtype, estimator=estimator\n",
    "        )\n",
    "        Y = validation.check_array(\n",
    "            Y, accept_sparse=\"csr\", dtype=dtype, estimator=estimator\n",
    "        )\n",
    "\n",
    "    if precomputed:\n",
    "        if X.shape[1] != Y.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Precomputed metric requires shape \"\n",
    "                \"(n_queries, n_indexed). Got (%d, %d) \"\n",
    "                \"for %d indexed.\" % (X.shape[0], X.shape[1], Y.shape[0])\n",
    "            )\n",
    "    elif X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"Incompatible dimension for X and Y matrices: \"\n",
    "            \"X.shape[1] == %d while Y.shape[1] == %d\" % (X.shape[1], Y.shape[1])\n",
    "        )\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# gower_distances: see header below for details.\n",
    "#\n",
    "# For purposes of this assignment you can simply call it like this on a dataset X (n instances):\n",
    "# D = gower_distances(X)\n",
    "#\n",
    "# which results an n x n distance matrix.\n",
    "#\n",
    "# Source: M. Beckmann  https://github.com/scikit-learn/scikit-learn/pull/9555\n",
    "\n",
    "\n",
    "def gower_distances(X, Y=None, feature_weight=None, categorical_features=None):\n",
    "    \"\"\"Computes the gower distances between X and Y\n",
    "\n",
    "    Gower is a similarity measure for categorical, boolean and numerical mixed\n",
    "    data.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, or pandas.DataFrame, shape (n_samples, n_features)\n",
    "\n",
    "    Y : array-like, or pandas.DataFrame, shape (n_samples, n_features)\n",
    "\n",
    "    feature_weight :  array-like, shape (n_features)\n",
    "        According the Gower formula, feature_weight is an attribute weight.\n",
    "\n",
    "    categorical_features: array-like, shape (n_features)\n",
    "        Indicates with True/False whether a column is a categorical attribute.\n",
    "        This is useful when categorical atributes are represented as integer\n",
    "        values. Categorical ordinal attributes are treated as numeric, and must\n",
    "        be marked as false.\n",
    "\n",
    "        Alternatively, the categorical_features array can be represented only\n",
    "        with the numerical indexes of the categorical attribtes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similarities : ndarray, shape (n_samples, n_samples)\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The non-numeric features, and numeric feature ranges are determined from X and not Y.\n",
    "    No support for sparse matrices.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if issparse(X) or issparse(Y):\n",
    "        raise TypeError(\"Sparse matrices are not supported for gower distance\")\n",
    "\n",
    "    y_none = Y is None\n",
    "\n",
    "    # It is necessary to convert to ndarray in advance to define the dtype\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "    # this is necessary as strangelly the validator is rejecting\n",
    "    #  numeric arrays with NaN\n",
    "    # array_type = np.object\n",
    "\n",
    "    # array_type = object replaces above due to deprecation of\n",
    "    # np.object.\n",
    "    array_type = object\n",
    "\n",
    "    if np.issubdtype(X.dtype, np.number) and (\n",
    "        np.isfinite(X.sum()) or np.isfinite(X).all()\n",
    "    ):\n",
    "        array_type = type(np.zeros(1, X.dtype).flat[0])\n",
    "\n",
    "    X, Y = check_pairwise_arrays(X, Y, precomputed=False, dtype=array_type)\n",
    "\n",
    "    n_rows, n_cols = X.shape\n",
    "\n",
    "    if categorical_features is None:\n",
    "        categorical_features = np.zeros(n_cols, dtype=bool)\n",
    "        for col in range(n_cols):\n",
    "            # In numerical columns, None is converted to NaN,\n",
    "            # and the type of NaN is recognized as a number subtype\n",
    "            if not np.issubdtype(type(X[0, col]), np.number):\n",
    "                categorical_features[col] = True\n",
    "    else:\n",
    "        categorical_features = np.array(categorical_features)\n",
    "\n",
    "    # if categorical_features.dtype == np.int32:\n",
    "    # np.int deprecated\n",
    "    # if np.issubdtype(categorical_features.dtype, np.int):\n",
    "\n",
    "    if np.issubdtype(categorical_features.dtype, np.int64):\n",
    "        new_categorical_features = np.zeros(n_cols, dtype=bool)\n",
    "        new_categorical_features[categorical_features] = True\n",
    "        categorical_features = new_categorical_features\n",
    "\n",
    "    # print(categorical_features)\n",
    "\n",
    "    # Categorical columns\n",
    "    X_cat = X[:, categorical_features]\n",
    "\n",
    "    # Numerical columns\n",
    "    X_num = X[:, np.logical_not(categorical_features)]\n",
    "    ranges_of_numeric = None\n",
    "    max_of_numeric = None\n",
    "\n",
    "    # Calculates the normalized ranges and max values of numeric values\n",
    "    _, num_cols = X_num.shape\n",
    "    ranges_of_numeric = np.zeros(num_cols)\n",
    "    max_of_numeric = np.zeros(num_cols)\n",
    "    for col in range(num_cols):\n",
    "        col_array = X_num[:, col].astype(np.float32)\n",
    "        max = np.nanmax(col_array)\n",
    "        min = np.nanmin(col_array)\n",
    "\n",
    "        if np.isnan(max):\n",
    "            max = 0.0\n",
    "        if np.isnan(min):\n",
    "            min = 0.0\n",
    "        max_of_numeric[col] = max\n",
    "        ranges_of_numeric[col] = (1 - min / max) if (max != 0) else 0.0\n",
    "\n",
    "    # This is to normalize the numeric values between 0 and 1.\n",
    "    X_num = np.divide(\n",
    "        X_num, max_of_numeric, out=np.zeros_like(X_num), where=max_of_numeric != 0\n",
    "    )\n",
    "\n",
    "    if feature_weight is None:\n",
    "        feature_weight = np.ones(n_cols)\n",
    "\n",
    "    feature_weight_cat = feature_weight[categorical_features]\n",
    "    feature_weight_num = feature_weight[np.logical_not(categorical_features)]\n",
    "\n",
    "    y_n_rows, _ = Y.shape\n",
    "\n",
    "    dm = np.zeros((n_rows, y_n_rows), dtype=np.float32)\n",
    "\n",
    "    feature_weight_sum = feature_weight.sum()\n",
    "\n",
    "    Y_cat = None\n",
    "    Y_num = None\n",
    "\n",
    "    if not y_none:\n",
    "        Y_cat = Y[:, categorical_features]\n",
    "        Y_num = Y[:, np.logical_not(categorical_features)]\n",
    "        # This is to normalize the numeric values between 0 and 1.\n",
    "        Y_num = np.divide(\n",
    "            Y_num, max_of_numeric, out=np.zeros_like(Y_num), where=max_of_numeric != 0\n",
    "        )\n",
    "    else:\n",
    "        Y_cat = X_cat\n",
    "        Y_num = X_num\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        j_start = i\n",
    "\n",
    "        # for non square results\n",
    "        if n_rows != y_n_rows:\n",
    "            j_start = 0\n",
    "\n",
    "        Y_cat[j_start:n_rows, :]\n",
    "        Y_num[j_start:n_rows, :]\n",
    "        result = _gower_distance_row(\n",
    "            X_cat[i, :],\n",
    "            X_num[i, :],\n",
    "            Y_cat[j_start:n_rows, :],\n",
    "            Y_num[j_start:n_rows, :],\n",
    "            feature_weight_cat,\n",
    "            feature_weight_num,\n",
    "            feature_weight_sum,\n",
    "            categorical_features,\n",
    "            ranges_of_numeric,\n",
    "            max_of_numeric,\n",
    "        )\n",
    "        dm[i, j_start:] = result\n",
    "        dm[i:, j_start] = result\n",
    "\n",
    "    return dm\n",
    "\n",
    "\n",
    "def _gower_distance_row(\n",
    "    xi_cat,\n",
    "    xi_num,\n",
    "    xj_cat,\n",
    "    xj_num,\n",
    "    feature_weight_cat,\n",
    "    feature_weight_num,\n",
    "    feature_weight_sum,\n",
    "    categorical_features,\n",
    "    ranges_of_numeric,\n",
    "    max_of_numeric,\n",
    "):\n",
    "    # categorical columns\n",
    "    sij_cat = np.where(xi_cat == xj_cat, np.zeros_like(xi_cat), np.ones_like(xi_cat))\n",
    "    sum_cat = np.multiply(feature_weight_cat, sij_cat).sum(axis=1)\n",
    "\n",
    "    # numerical columns\n",
    "    abs_delta = np.absolute(xi_num - xj_num)\n",
    "    sij_num = np.divide(\n",
    "        abs_delta,\n",
    "        ranges_of_numeric,\n",
    "        out=np.zeros_like(abs_delta),\n",
    "        where=ranges_of_numeric != 0,\n",
    "    )\n",
    "\n",
    "    sum_num = np.multiply(feature_weight_num, sij_num).sum(axis=1)\n",
    "    sums = np.add(sum_cat, sum_num)\n",
    "    sum_sij = np.divide(sums, feature_weight_sum)\n",
    "    return sum_sij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40afb940ab97cef93073c975fe1f6dcb",
     "grade": false,
     "grade_id": "cell-edebc844137b7a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t4a'></a>\n",
    "### Task 4a -  Using the Gower distances in the matrix D computed by the provided function, find the most and least similar pairs of colleges in the dataset (15 points).  \n",
    "\n",
    "Note that an item is most similar to itself (distance = 0.) but you need to disallow this case since we actually care about finding two *distinct* items not along the diagonal that are most similar. One quick way to accomplish this is to replace the zeros along the diagonal of the distance matrix D returned by the gower_distances function, with a very large number (e.g. 1000) that wouldn't occur as a distance in practice.\n",
    "\n",
    "You may also find numpy's `unravel_index` function, in combination with `argmax` or `argmin`, useful for finding min/max elements in an array. Remember that the least similar elements will have maximum distance from each other, and most similar will have minimum distance.\n",
    "\n",
    "Your function should return a 2-element tuple, consisting itself of two tuples: the first tuple should be the names (via the College.Name field) of the two colleges that are *least* similar according the Gower distance. The second tuple should name the *most* similar colleges.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64dd9910399128507bceafcea0f1f4b9",
     "grade": true,
     "grade_id": "cell-c512152e32067ed5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "task_id = \"4a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College.Name</th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   College.Name Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "0  Abilene Christian University     Yes  1660    1232     721         23   \n",
       "1            Adelphi University     Yes  2186    1924     512         16   \n",
       "2                Adrian College     Yes  1428    1097     336         22   \n",
       "3           Agnes Scott College     Yes   417     349     137         60   \n",
       "4     Alaska Pacific University     Yes   193     146      55         16   \n",
       "\n",
       "   Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  Personal  \\\n",
       "0         52         2885          537      7440        3300    450      2200   \n",
       "1         29         2683         1227     12280        6450    750      1500   \n",
       "2         50         1036           99     11250        3750    400      1165   \n",
       "3         89          510           63     12960        5450    450       875   \n",
       "4         44          249          869      7560        4120    800      1500   \n",
       "\n",
       "   PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0   70        78       18.1           12    7041         60  \n",
       "1   29        30       12.2           16   10527         56  \n",
       "2   53        66       12.9           30    8735         54  \n",
       "3   92        97        7.7           37   19016         59  \n",
       "4   76        72       11.9            2   10922         15  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a648ee283beab98a9708cb63d3fb7730",
     "grade": false,
     "grade_id": "cell-06feac2706761a09",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_mixed_features_a():\n",
    "    result = None\n",
    "\n",
    "    dis = gower_distances(df)\n",
    "\n",
    "    # Replace diagonal elements with a large number to ignore self-similarity\n",
    "    min_met = dis + np.diag(np.ones(dis.shape[0])*100)\n",
    "\n",
    "    # Find indices of the most similar pair (minimum distance)\n",
    "    min_pair, _ = np.where(min_met == np.min(min_met))\n",
    "    \n",
    "    # Find indices of the least similar pair (maximum distance)\n",
    "    max_pair, _ = np.where(dis == np.max(dis))\n",
    "\n",
    "    result = ((df['College.Name'][max_pair[0]],df['College.Name'][max_pair[1]]),\\\n",
    "     (df['College.Name'][min_pair[0]],df['College.Name'][min_pair[1]]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0a220076dab9ed534a7bc868c37e933",
     "grade": true,
     "grade_id": "cell-c2e6dd563f89e13c",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4a - AG tests\n",
      "Task 4a - your answer:\n",
      "(('Center for Creative Studies', 'Texas A&M Univ. at College Station'), ('Augustana College IL', 'Hope College'))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_mixed_features_a()\n",
    "\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 4a: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Task 4a: Your return tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[0], tuple), \"Task 4a: The first element should be a tuple. \"\n",
    "assert len(stu_ans[0]) == 2, \"Task 4a: The first tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[1], tuple), \"Task 4a: The second element should be a tuple. \"\n",
    "assert len(stu_ans[1]) == 2, \"Task 4a: The second tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[0][0], str), \"Task 4a: Element [0][0] must be a string.\"\n",
    "assert isinstance(stu_ans[0][1], str), \"Task : Element [0][0] must be a string.\"\n",
    "assert isinstance(stu_ans[1][0], str), \"Task 4a: Element [1][0] must be a string.\"\n",
    "assert isinstance(stu_ans[1][1], str), \"Task 4a: Element [1][1] must be a string.\"\n",
    "\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2558578d1e7b0077db2da9c073f27450",
     "grade": false,
     "grade_id": "cell-5c6da0d378a6c59a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t4b'></a>\n",
    "### Task 4b - With the dissimilarity matrix you computed in Part A, now run k-medoids clustering with 3 clusters, and return the central representative found by k-medoids for each cluster (10 points).\n",
    "\n",
    "Your function should return a dataframe with 3 rows corresponding to the k-medoids representatives for each cluster, and have columns named \"College.Name\", \"Private\", \"Enroll\",\"Expend\", and \"Grad.Rate\" containing those values of the representative college.\n",
    "\n",
    "NOTE: Call KMedoids using the metric = 'precomputed' option, and random_state = 42. Make sure your dataframe is ordered in the same order as the medoid indices that are returned by kmedoids.medoid_indices_.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "674cdb712667a79a76f79d9b1e656c82",
     "grade": true,
     "grade_id": "cell-491eabab477b3211",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "task_id = \"4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84cc3381f5c9b457d9fd0a5ec6872244",
     "grade": false,
     "grade_id": "cell-6adf33ccf58ba453",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_mixedfeatures_b():\n",
    "    result = None\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "    dis = gower_distances(df)\n",
    "    kmedoids = KMedoids(n_clusters=3, metric='precomputed', random_state=42)\n",
    "    kmedoids.fit(dis)\n",
    "    medoid_indices = kmedoids.medoid_indices_\n",
    "    result = df.iloc[medoid_indices][['College.Name','Private','Enroll','Expend','Grad.Rate']].reset_index(drop= True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a58b8c31819c67504ffb36c370ae5a25",
     "grade": true,
     "grade_id": "cell-3d425130ecc2d2d3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4b - AG tests\n",
      "Task 4b - your answer:\n",
      "                                 College.Name Private  Enroll  Expend  \\\n",
      "0                           Alfred University     Yes     472   10932   \n",
      "1                      Westminster College MO     Yes     184    7925   \n",
      "2  University of North Carolina at Wilmington      No    1449    6005   \n",
      "\n",
      "   Grad.Rate  \n",
      "0         73  \n",
      "1         62  \n",
      "2         55  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_mixedfeatures_b()\n",
    "\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(\n",
    "    stu_ans, pd.DataFrame\n",
    "), \"Task 4b: Your function should return a pandas DataFrame. \"\n",
    "assert stu_ans.shape == (3, 5), \"Task 4b: The shape of your dataframe isn't correct. \"\n",
    "\n",
    "assert list(stu_ans.columns) == [\n",
    "    \"College.Name\",\n",
    "    \"Private\",\n",
    "    \"Enroll\",\n",
    "    \"Expend\",\n",
    "    \"Grad.Rate\",\n",
    "], \"Task 4b: Please check the column names of your DataFrame.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#toc'>TOC</a>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_unsupervised_learning_v1_assignment2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
